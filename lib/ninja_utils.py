#!/usr/bin/env python3
"""Utilities for interacting with Ninja build system."""

import os
import sys
import re
import subprocess
import logging
import json
import hashlib
from pathlib import Path
from typing import List, Tuple, Dict, Set, Optional
from collections import defaultdict

from lib.constants import (
    COMPILE_COMMANDS_JSON,
    EXIT_INVALID_ARGS,
    EXIT_NINJA_FAILED,
    EXIT_RUNTIME_ERROR,
    BuildDirectoryError,
    NinjaError
)
from lib.color_utils import Colors, print_error, print_warning, print_success

logger = logging.getLogger(__name__)

# Cache file for tracking generated file hashes
GENERATED_FILES_CACHE = '.buildcheck_generated_cache.json'

RE_NINJA_EXPLAIN = re.compile(r"ninja explain: (.*)")
RE_RECENT_INPUT = re.compile(r'most recent input\s+([^\s\(]+)')


def check_ninja_available() -> bool:
    """Check if ninja is available in PATH.
    
    Returns:
        True if ninja is available, False otherwise
    """
    try:
        result = subprocess.run(
            ["ninja", "--version"],
            capture_output=True,
            check=True,
            timeout=5
        )
        logger.debug(f"Found ninja: {result.stdout.decode().strip()}")
        return True
    except FileNotFoundError:
        logger.debug("ninja not found in PATH")
        return False
    except subprocess.TimeoutExpired:
        logger.warning("ninja --version timed out after 5 seconds")
        return False
    except subprocess.CalledProcessError as e:
        logger.warning(f"ninja --version failed with exit code {e.returncode}")
        return False
    except Exception as e:
        logger.warning(f"Unexpected error checking for ninja: {e}")
        return False


def validate_build_directory(build_dir: str) -> Path:
    """Validate that the build directory exists and contains a ninja build file.
    
    Args:
        build_dir: Path to the build directory
        
    Returns:
        Validated Path object
        
    Raises:
        ValueError: If directory is invalid or doesn't contain build.ninja
    """
    path = Path(build_dir).resolve()
    
    if not path.exists():
        raise ValueError(f"Build directory does not exist: {build_dir}")
    
    if not path.is_dir():
        raise ValueError(f"Path is not a directory: {build_dir}")
    
    build_ninja = path / "build.ninja"
    if not build_ninja.exists():
        raise ValueError(
            f"No build.ninja found in {build_dir}. "
            "Please provide a valid ninja build directory."
        )
    
    return path


def parse_ninja_generated_files(build_ninja_path: str) -> Set[str]:
    """Parse build.ninja to identify files that are generated by build rules.
    
    Generated files are identified by:
    - Being outputs of CUSTOM_COMMAND rules (CMake generated files)
    - Being outputs of rules with 'generator = 1' flag
    - Being outputs of code generation rules
    
    Args:
        build_ninja_path: Path to build.ninja file
        
    Returns:
        Set of generated file paths (relative to build directory)
    """
    generated_files: Set[str] = set()
    
    # Track generator rules (rules with generator = 1)
    generator_rules: Set[str] = set()
    current_rule: Optional[str] = None
    
    # Regex patterns
    rule_pattern = re.compile(r'^rule\s+(\S+)')
    generator_pattern = re.compile(r'^\s+generator\s*=\s*1')
    build_pattern = re.compile(r'^build\s+([^:]+):\s+(\S+)')
    
    logger.debug(f"Parsing {build_ninja_path} to identify generated files...")
    
    try:
        with open(build_ninja_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.rstrip()
                
                # Check for rule definition
                rule_match = rule_pattern.match(line)
                if rule_match:
                    current_rule = rule_match.group(1)
                    continue
                
                # Check if current rule is a generator rule
                if current_rule and generator_pattern.match(line):
                    generator_rules.add(current_rule)
                    logger.debug(f"Found generator rule: {current_rule}")
                    continue
                
                # Check for build statements
                build_match = build_pattern.match(line)
                if build_match:
                    outputs_str = build_match.group(1)
                    rule_name = build_match.group(2)
                    
                    # Mark outputs as generated if:
                    # 1. Rule is a generator rule
                    # 2. Rule is CUSTOM_COMMAND (CMake generated files)
                    # 3. Rule is not a C/C++ compilation rule
                    if (rule_name in generator_rules or 
                        'CUSTOM_COMMAND' in rule_name or
                        rule_name in {'phony', 'RERUN_CMAKE'}):
                        
                        # Parse output files (handle | for implicit outputs)
                        outputs = outputs_str.split('|')[0].strip().split()
                        for output in outputs:
                            output = output.strip()
                            if output and not output.endswith('.stamp'):
                                generated_files.add(output)
                                logger.debug(f"Identified generated file: {output} (rule: {rule_name})")
    
    except IOError as e:
        logger.warning(f"Failed to read {build_ninja_path}: {e}")
        return set()
    
    logger.info(f"Found {len(generated_files)} generated files in build.ninja")
    return generated_files


def compute_file_hash(file_path: str) -> str:
    """Compute SHA256 hash of a file.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Hexadecimal hash string
    """
    sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                sha256.update(chunk)
        return sha256.hexdigest()
    except IOError as e:
        logger.warning(f"Failed to hash {file_path}: {e}")
        return ""


def load_generated_files_cache(cache_path: str) -> Dict[str, str]:
    """Load the cache of generated file hashes.
    
    Args:
        cache_path: Path to the cache file
        
    Returns:
        Dictionary mapping file paths to their hashes
    """
    if not os.path.exists(cache_path):
        return {}
    
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            data: Dict[str, str] = json.load(f)
            return data
    except (IOError, json.JSONDecodeError) as e:
        logger.warning(f"Failed to load generated files cache: {e}")
        return {}


def save_generated_files_cache(cache_path: str, cache: Dict[str, str]) -> None:
    """Save the cache of generated file hashes.
    
    Args:
        cache_path: Path to the cache file
        cache: Dictionary mapping file paths to their hashes
    """
    try:
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2)
    except IOError as e:
        logger.warning(f"Failed to save generated files cache: {e}")


def check_generated_files_changed(
    build_dir: str, 
    generated_files: Set[str], 
    cache: Dict[str, str]
) -> Tuple[List[str], List[str]]:
    """Check if generated files are missing or have changed since last cache.
    
    Args:
        build_dir: Path to the build directory
        generated_files: Set of generated file paths (relative to build_dir)
        cache: Cache dictionary with file path -> hash mappings
        
    Returns:
        Tuple of (missing_files, changed_files)
    """
    missing_files = []
    changed_files = []
    
    for gen_file in generated_files:
        # Only check source files (cpp, c, etc) not intermediate files
        if not any(gen_file.endswith(ext) for ext in ['.cpp', '.cc', '.cxx', '.c', '.h', '.hpp', '.hxx']):
            continue
        
        full_path = os.path.join(build_dir, gen_file) if not os.path.isabs(gen_file) else gen_file
        
        if not os.path.exists(full_path):
            missing_files.append(full_path)
            logger.debug(f"Missing generated file: {full_path}")
        else:
            # Check if hash has changed (only if file was previously cached)
            cached_hash = cache.get(full_path)
            
            if cached_hash is not None:
                # File was in cache, check if it changed
                current_hash = compute_file_hash(full_path)
                if current_hash != cached_hash:
                    changed_files.append(full_path)
                    logger.debug(f"Generated file changed: {full_path}")
            # If cached_hash is None, this is a new file - don't mark as changed
    
    return missing_files, changed_files


def update_generated_files_cache(
    cache: Dict[str, str], 
    files_to_update: List[str], 
    cache_path: str
) -> None:
    """Update the cache with current hashes of specific generated files.
    
    Args:
        cache: Current cache dictionary to update
        files_to_update: List of file paths to update in cache
        cache_path: Path to save the cache file
    """
    for full_path in files_to_update:
        if os.path.exists(full_path):
            cache[full_path] = compute_file_hash(full_path)
    
    save_generated_files_cache(cache_path, cache)


def clean_stale_cache_entries(
    cache: Dict[str, str],
    current_generated_files: Set[str],
    build_dir: str
) -> Dict[str, str]:
    """Remove cache entries for files that are no longer generated or don't exist.
    
    Args:
        cache: Current cache dictionary
        current_generated_files: Set of currently generated file paths (relative to build_dir)
        build_dir: Build directory path
        
    Returns:
        Cleaned cache dictionary with only valid entries
    """
    # Build set of expected absolute paths from current generated files
    expected_paths = set()
    for gen_file in current_generated_files:
        # Only track source files
        if any(gen_file.endswith(ext) for ext in ['.cpp', '.cc', '.cxx', '.c', '.h', '.hpp', '.hxx']):
            full_path = os.path.join(build_dir, gen_file) if not os.path.isabs(gen_file) else gen_file
            expected_paths.add(full_path)
    
    # Create new cache with only valid entries
    cleaned_cache = {}
    removed_count = 0
    
    for file_path, file_hash in cache.items():
        # Keep entry if file is still in generated files list and exists
        if file_path in expected_paths and os.path.exists(file_path):
            cleaned_cache[file_path] = file_hash
        else:
            removed_count += 1
            logger.debug(f"Removing stale cache entry: {file_path}")
    
    if removed_count > 0:
        logger.info(f"Cleaned {removed_count} stale entries from generated files cache")
    
    return cleaned_cache


def check_missing_source_files(compile_commands_path: str) -> List[str]:
    """Check if any source files referenced in compile_commands.json are missing.
    
    This detects when autogenerated files haven't been built yet.
    
    Args:
        compile_commands_path: Path to compile_commands.json
        
    Returns:
        List of missing file paths (empty if all files exist)
        
    Raises:
        RuntimeError: If compile_commands.json is invalid
    """
    try:
        with open(compile_commands_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (IOError, json.JSONDecodeError) as e:
        raise RuntimeError(f"Failed to read compile_commands.json: {e}") from e
    
    if not isinstance(data, list):
        raise RuntimeError(f"Invalid compile_commands.json format: expected list, got {type(data)}")
    
    missing_files = []
    for entry in data:
        if not isinstance(entry, dict):
            continue
        
        file_path = entry.get('file')
        if not file_path:
            continue
        
        # Handle relative paths by resolving against the build directory
        directory = entry.get('directory', '')
        if directory and not os.path.isabs(file_path):
            full_path = os.path.join(directory, file_path)
        else:
            full_path = file_path
        
        if not os.path.exists(full_path):
            missing_files.append(full_path)
            logger.debug(f"Missing source file: {full_path}")
    
    return missing_files


def run_full_ninja_build(build_dir: str, verbose: bool = False, timeout: int = 3600, targets: Optional[List[str]] = None) -> bool:
    """Run a ninja build to generate autogenerated files.
    
    Args:
        build_dir: Path to the build directory
        verbose: Whether to print progress messages
        timeout: Command timeout in seconds (default: 3600 = 1 hour)
        targets: Optional list of specific targets to build. If None, builds all.
        
    Returns:
        True if build succeeded, False otherwise
    """
    if targets:
        logger.info(f"Running ninja build for {len(targets)} targets in {build_dir}")
        if verbose:
            print_warning(f"Building {len(targets)} autogenerated files...", prefix=False)
    else:
        logger.info(f"Running full ninja build in {build_dir}")
        if verbose:
            print_warning(f"Running full ninja build to generate autogenerated files...", prefix=False)
    
    try:
        # Build command: ninja [target1 target2 ...]
        cmd = ["ninja"]
        if targets:
            cmd.extend(targets)
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=build_dir,
            timeout=timeout
        )
        
        if result.returncode == 0:
            logger.info("Ninja build completed successfully")
            if verbose:
                print_success("Build completed successfully")
            return True
        else:
            logger.warning(f"Ninja build failed with exit code {result.returncode}")
            if verbose:
                print_error(f"Build failed with exit code {result.returncode}", prefix=False)
            if result.stderr:
                logger.debug(f"Build stderr: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        logger.error(f"Ninja build timed out after {timeout} seconds")
        if verbose:
            print_error(f"Build timed out after {timeout} seconds", prefix=False)
        return False
    except FileNotFoundError:
        logger.error("ninja command not found")
        if verbose:
            print_error("ninja command not found in PATH", prefix=False)
        return False
    except Exception as e:
        logger.error(f"Unexpected error running ninja build: {e}")
        if verbose:
            print_error(f"Unexpected error: {e}", prefix=False)
        return False


def get_relative_build_path(full_path: str, build_dir: str) -> str:
    """Convert an absolute file path to a path relative to build directory.
    
    This is needed because ninja expects targets relative to the build directory.
    
    Args:
        full_path: Absolute path to a file
        build_dir: Build directory path
        
    Returns:
        Path relative to build directory, or the original path if conversion fails
    """
    try:
        rel_path = os.path.relpath(full_path, build_dir)
        # If path goes outside build_dir, return the basename
        if rel_path.startswith('..'):
            return os.path.basename(full_path)
        return rel_path
    except ValueError:
        # On Windows, relpath can fail if paths are on different drives
        return os.path.basename(full_path)


def validate_and_prepare_build_dir(build_dir: str, verbose: bool = False) -> Tuple[str, str]:
    """Validate build directory and ensure compile_commands.json exists.
    
    Args:
        build_dir: Path to ninja build directory
        verbose: Whether to print progress messages
    
    Returns:
        Tuple of (validated build_dir path, compile_commands.json path)
    
    Raises:
        ValueError: If build_dir is invalid or path traversal detected
        RuntimeError: If compile_commands.json generation fails
    """
    build_dir = os.path.realpath(os.path.abspath(build_dir))
    if not os.path.isdir(build_dir):
        raise ValueError(f"Build directory does not exist: {build_dir}")
    
    compile_commands = os.path.realpath(os.path.join(build_dir, COMPILE_COMMANDS_JSON))
    build_ninja = os.path.realpath(os.path.join(build_dir, 'build.ninja'))
    
    # Validate paths are within build_dir (protect against symlink attacks)
    try:
        rel_path = os.path.relpath(compile_commands, build_dir)
        if rel_path.startswith('..'):
            raise ValueError()
    except (ValueError, OSError):
        raise ValueError(f"Path traversal detected: {COMPILE_COMMANDS_JSON}")
    
    # Parse build.ninja to identify generated files
    generated_files = parse_ninja_generated_files(build_ninja)
    
    # Load cache of previous generated file hashes
    cache_path = os.path.join(build_dir, GENERATED_FILES_CACHE)
    cache = load_generated_files_cache(cache_path)
    
    # Clean stale entries from cache (removed files)
    cache = clean_stale_cache_entries(cache, generated_files, build_dir)
    
    # Check if generated files are missing or changed
    missing_generated, changed_generated = check_generated_files_changed(
        build_dir, generated_files, cache
    )
    
    # If any generated files are missing or changed, build only those files
    files_to_build = list(set(missing_generated + changed_generated))
    
    if files_to_build:
        logger.info(
            f"Found {len(missing_generated)} missing and {len(changed_generated)} changed generated files"
        )
        if verbose:
            print_warning(f"Detected {len(files_to_build)} autogenerated files needing rebuild...", prefix=False)
        
        # Convert absolute paths to build-relative paths for ninja
        targets = [get_relative_build_path(f, build_dir) for f in files_to_build]
        
        if not run_full_ninja_build(build_dir, verbose=verbose, targets=targets):
            logger.warning("Ninja build failed, but continuing anyway")
        
        # Update cache with new hashes after build
        update_generated_files_cache(cache, files_to_build, cache_path)
    else:
        logger.info("No changes detected in generated files")
        if verbose:
            print_success("Autogenerated files are up to date")
    
    # Determine if we need to regenerate compile_commands.json
    need_build = len(files_to_build) > 0
    
    # Generate compile_commands.json if needed
    needs_regeneration = (not os.path.exists(compile_commands) or 
                         (os.path.exists(build_ninja) and 
                          os.path.getmtime(build_ninja) > os.path.getmtime(compile_commands)) or
                         need_build)
    
    if needs_regeneration:
        logger.info("Generating compile_commands.json...")
        if verbose:
            print(f"{Colors.CYAN}Generating compile_commands.json...{Colors.RESET}")
        try:
            result = subprocess.run(
                ["ninja", "-t", "compdb"],
                capture_output=True,
                text=True,
                cwd=build_dir,
                check=True
            )
            with open(compile_commands, 'w', encoding='utf-8') as f:
                f.write(result.stdout)
            logger.debug(f"Generated: {compile_commands}")
            
            # Update cache after successful generation with all generated files
            if generated_files:
                # Convert set to list for all generated files
                all_generated = [os.path.join(build_dir, f) if not os.path.isabs(f) else f 
                                 for f in generated_files 
                                 if any(f.endswith(ext) for ext in ['.cpp', '.cc', '.cxx', '.c', '.h', '.hpp', '.hxx'])]
                update_generated_files_cache(cache, all_generated, cache_path)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Failed to generate compile_commands.json: {e.stderr}") from e
        except IOError as e:
            raise IOError(f"Failed to write compile_commands.json: {e}") from e
    
    if not os.path.isfile(compile_commands):
        raise ValueError(f"compile_commands.json not found in {build_dir}")
    
    return build_dir, compile_commands


def validate_build_directory_with_feedback(build_directory: str, verbose: bool = False) -> Tuple[str, str]:
    """Validate and prepare build directory with user-friendly error messages.
    
    This is a wrapper around validate_and_prepare_build_dir that provides
    standardized error handling and user feedback for command-line scripts.
    
    Args:
        build_directory: Path to build directory to validate
        verbose: Whether to print verbose messages
    
    Returns:
        Tuple of (validated build_dir path, compile_commands.json path)
    
    Raises:
        ValueError: If validation fails
        RuntimeError: If preparation fails
    """
    try:
        # Check for build.ninja first (more specific error message)
        build_dir_check = os.path.realpath(os.path.abspath(build_directory))
        build_ninja = os.path.realpath(os.path.join(build_dir_check, 'build.ninja'))
        if not os.path.isfile(build_ninja):
            error_msg = f"build.ninja not found in '{build_dir_check}'"
            logger.error(error_msg)
            print_error(error_msg)
            print_warning("Hint: This script requires a Ninja build directory", prefix=False)
            raise ValueError(error_msg)
        
        # Validate and prepare build directory
        build_dir, compile_commands = validate_and_prepare_build_dir(build_directory, verbose=verbose)
        logger.info(f"Using build directory: {build_dir}")
        return build_dir, compile_commands
    except ValueError as e:
        logger.error(f"Build directory validation failed: {e}")
        print_error(str(e))
        raise
    except RuntimeError as e:
        logger.error(f"Failed to prepare build directory: {e}")
        print_error(str(e))
        raise
    except Exception as e:
        logger.exception(f"Error validating build directory: {e}")
        print_error(str(e))
        raise RuntimeError(f"Error validating build directory: {e}") from e


def run_ninja_explain(build_dir: Path, timeout: int = 300) -> subprocess.CompletedProcess[str]:
    """Run ninja -n -d explain to get rebuild information.
    
    Args:
        build_dir: Path to the build directory
        timeout: Command timeout in seconds (default: 300)
        
    Returns:
        CompletedProcess object with ninja output
        
    Raises:
        RuntimeError: If ninja execution fails
    """
    logger.info(f"Running ninja -n -d explain in {build_dir}")
    
    try:
        result = subprocess.run(
            ["ninja", "-n", "-d", "explain"],
            capture_output=True,
            text=True,
            cwd=str(build_dir),
            timeout=timeout
        )
        return result
    except FileNotFoundError:
        raise RuntimeError(
            "ninja command not found. Please ensure ninja is installed and in PATH."
        )
    except subprocess.TimeoutExpired:
        raise RuntimeError(f"ninja command timed out after {timeout} seconds")
    except Exception as e:
        raise RuntimeError(f"Unexpected error running ninja: {e}")


def parse_ninja_explain_output(stderr_lines: List[str]) -> Tuple[List[str], Set[str]]:
    """Parse ninja explain output to extract rebuild targets and changed files.
    
    Args:
        stderr_lines: Lines from ninja stderr output
        
    Returns:
        Tuple of (rebuild_targets, changed_files)
    """
    rebuild_targets = []
    changed_files = set()
    
    for line in stderr_lines:
        match = RE_NINJA_EXPLAIN.search(line)
        if not match:
            continue
        
        explain_msg = match.group(1)
        
        # Skip "is dirty" lines as they're redundant
        if "is dirty" in explain_msg:
            continue
        
        # Extract target from the message
        if explain_msg.startswith("output "):
            parts = explain_msg.split(" ", 2)
            if len(parts) > 1:
                target = parts[1]
                rebuild_targets.append(target)
        elif "command line changed for " in explain_msg:
            target = explain_msg.split("command line changed for ", 1)[1]
            rebuild_targets.append(target)
        
        # Extract changed files (headers)
        input_match = RE_RECENT_INPUT.search(explain_msg)
        if input_match:
            changed_file = input_match.group(1)
            changed_files.add(changed_file)
    
    return rebuild_targets, changed_files


def extract_rebuild_info(build_dir: str, verbose: bool = False) -> Tuple[List[Tuple[str, str]], Dict[str, int], Dict[str, int]]:
    """Extract rebuild information from ninja explain output.
    
    Args:
        build_dir: Path to the ninja build directory
        verbose: If True, print detailed progress information
    
    Returns:
        tuple: (rebuild_entries, reasons, root_causes) where:
            - rebuild_entries: list of (output_file, reason) tuples
            - reasons: dict mapping normalized reason to count
            - root_causes: dict mapping changed file to rebuild count
            
    Raises:
        BuildDirectoryError: If build directory is invalid or inaccessible
        NinjaError: If ninja command fails, times out, or is not found
        RuntimeError: If an unexpected error occurs
    """
    if verbose:
        print(f"Analyzing build directory: {build_dir}", file=sys.stderr)
    
    # Save current directory and change to build directory
    original_dir = os.getcwd()
    try:
        os.chdir(build_dir)
    except OSError as e:
        print(f"Error: Cannot change to directory '{build_dir}': {e}", file=sys.stderr)
        raise BuildDirectoryError(f"Cannot change to directory '{build_dir}': {e}")

    try:
        result = subprocess.run(
            ["ninja", "-n", "-d", "explain"],
            capture_output=True,
            text=True,
            check=True,
            timeout=300
        )
    except FileNotFoundError:
        print("Error: 'ninja' command not found. Please ensure ninja is installed and in PATH.", file=sys.stderr)
        raise NinjaError("ninja command not found. Please ensure ninja is installed and in PATH.")
    except subprocess.TimeoutExpired:
        print("Error: Ninja command timed out after 5 minutes.", file=sys.stderr)
        raise NinjaError("Ninja command timed out after 5 minutes")
    except subprocess.CalledProcessError as e:
        print(f"Error: Ninja command failed with exit code {e.returncode}", file=sys.stderr)
        if e.stderr:
            print(f"Stderr output:\n{e.stderr}", file=sys.stderr)
        raise NinjaError(f"Ninja command failed with exit code {e.returncode}")
    except Exception as e:
        print(f"Unexpected error running ninja: {e}", file=sys.stderr)
        raise RuntimeError(f"Unexpected error running ninja: {e}")
    finally:
        os.chdir(original_dir)

    lines = result.stderr.splitlines()

    if verbose:
        print(f"Processing {len(lines)} lines of ninja output", file=sys.stderr)

    rebuild_entries = []
    reasons: Dict[str, int] = defaultdict(int)
    root_causes: Dict[str, int] = defaultdict(int)

    for line in lines:
        m = RE_NINJA_EXPLAIN.search(line)
        if not m:
            continue

        explain_msg = m.group(1)
        
        if "is dirty" in explain_msg:
            continue
        
        output_file = "unknown"
        if explain_msg.startswith("output "):
            parts = explain_msg.split(" ", 2)
            if len(parts) > 1:
                output_file = parts[1]
        elif "command line changed for " in explain_msg:
            output_file = explain_msg.split("command line changed for ", 1)[1]
        
        reason_norm = normalize_reason(explain_msg)
        rebuild_entries.append((output_file, reason_norm))
        reasons[reason_norm] += 1

        m2 = re.search(r"([^\s]+\.h\w*)", explain_msg)
        if m2:
            root_causes[m2.group(1)] += 1
    
    return rebuild_entries, reasons, root_causes


def normalize_reason(msg: str) -> str:
    """Normalize a ninja explain message into a human-readable rebuild reason.
    
    Args:
        msg: Raw ninja explain message
        
    Returns:
        Normalized, user-friendly reason string
    """
    if not msg:
        return "unknown reason"
    
    msg_lower = msg.lower()

    if "output missing" in msg_lower or "doesn't exist" in msg_lower:
        return "output missing (initial build)"
    if "older than most recent input" in msg_lower:
        return "input source changed"
    if "command line changed" in msg_lower:
        return "command line changed (compile flags/options)"
    if "input" in msg_lower and "newer" in msg_lower:
        return "input source changed"
    if "depfile" in msg_lower:
        return "header dependency changed"
    if "build.ninja" in msg_lower:
        return "build.ninja changed (cmake reconfigure)"
    if "rule changed" in msg_lower:
        return "rule changed (compile flags/options)"
    if "is dirty" in msg_lower:
        return "[marked dirty]"

    return msg


def get_dependencies(build_dir: str, target: str, timeout: int = 30) -> List[str]:
    """Get dependencies for a target using ninja -t deps.
    
    Args:
        build_dir: Path to the build directory
        target: The build target to get dependencies for
        timeout: Command timeout in seconds (default: 30)
        
    Returns:
        List of dependency file paths
        
    Raises:
        RuntimeError: If ninja command fails or times out
    """
    try:
        result = subprocess.run(
            ["ninja", "-t", "deps", target],
            capture_output=True,
            text=True,
            cwd=build_dir,
            timeout=timeout
        )
        
        if result.returncode != 0:
            logger.warning(f"ninja -t deps failed for {target}: {result.stderr}")
            return []
        
        # Parse the output
        deps = []
        for line in result.stdout.splitlines():
            line = line.strip()
            # Skip target line (ends with :), empty lines, and comments
            if line and not line.endswith(':') and not line.startswith('#'):
                deps.append(line)
        
        return deps
        
    except subprocess.TimeoutExpired:
        logger.error(f"ninja -t deps timed out for {target}")
        return []
    except subprocess.CalledProcessError as e:
        logger.error(f"ninja -t deps failed for {target}: {e}")
        return []
    except Exception as e:
        logger.error(f"Unexpected error getting dependencies for {target}: {e}")
        return []


def generate_compile_commands(build_dir: str, timeout: int = 60) -> bool:
    """Generate compile_commands.json using ninja -t compdb.
    
    Args:
        build_dir: Path to the build directory
        timeout: Command timeout in seconds (default: 60)
        
    Returns:
        True if successful, False otherwise
    """
    compile_db = os.path.join(build_dir, COMPILE_COMMANDS_JSON)
    
    try:
        logger.info("Generating compile_commands.json...")
        result = subprocess.run(
            ["ninja", "-t", "compdb"],
            capture_output=True,
            text=True,
            cwd=build_dir,
            check=True,
            timeout=timeout
        )
        
        with open(compile_db, 'w', encoding='utf-8') as f:
            f.write(result.stdout)
        
        logger.debug(f"Generated: {compile_db}")
        return True
        
    except subprocess.TimeoutExpired:
        logger.error(f"ninja -t compdb timed out after {timeout} seconds")
        return False
    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to generate compile_commands.json: {e.stderr}")
        return False
    except IOError as e:
        logger.error(f"Failed to write compile_commands.json: {e}")
        return False
    except Exception as e:
        logger.error(f"Unexpected error generating compile_commands.json: {e}")
        return False
