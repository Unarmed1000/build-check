#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ****************************************************************************************************************************************************
# * BSD 3-Clause License
# *
# * Copyright (c) 2025, Mana Battery
# * All rights reserved.
# *
# * Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
# *
# * 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
# * 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the
# *    documentation and/or other materials provided with the distribution.
# * 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this
# *    software without specific prior written permission.
# *
# * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
# * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
# * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
# * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
# ****************************************************************************************************************************************************
"""Utilities for interacting with Ninja build system."""

import os
import sys
import re
import subprocess
import logging
import json
import hashlib
from pathlib import Path
from typing import List, Tuple, Dict, Set, Optional, Any
from collections import defaultdict
from dataclasses import dataclass

from lib.constants import COMPILE_COMMANDS_JSON, BuildDirectoryError, NinjaError
from lib.color_utils import Colors, print_error, print_warning, print_success
from lib.tool_detection import find_ninja

logger = logging.getLogger(__name__)

# Cache file for tracking generated file hashes
GENERATED_FILES_CACHE = ".buildcheck_generated_cache.json"

RE_NINJA_EXPLAIN = re.compile(r"ninja explain: (.*)")
RE_RECENT_INPUT = re.compile(r"most recent input\s+([^\s\(]+)")


@dataclass
class GeneratedFileInfo:
    """Information about a generated file and its dependencies.

    Attributes:
        outputs: Files generated by this rule instance
        explicit_inputs: Files the generator reads (.proto, .xml, .in)
        implicit_inputs: Generator scripts and headers (after |)
        order_only_inputs: Build-order dependencies (after ||)
        rule_name: Rule that generates these files
    """

    outputs: Set[str]
    explicit_inputs: Set[str]
    implicit_inputs: Set[str]
    order_only_inputs: Set[str]
    rule_name: str


def is_trackable_file(filepath: str) -> Tuple[bool, str]:
    """Check if a file should be tracked for changes.

    Args:
        filepath: Path to check

    Returns:
        Tuple of (is_trackable, category) where category is one of:
        'source', 'template', 'script', 'intermediate', or 'excluded'
    """
    # Normalize to lowercase for case-insensitive matching
    lower_path = filepath.lower()
    basename = os.path.basename(lower_path)

    # Generated source files
    source_exts = (".cpp", ".cc", ".cxx", ".c", ".h", ".hpp", ".hxx", ".hh")
    if lower_path.endswith(source_exts):
        return True, "source"

    # Generator input templates
    template_exts = (".proto", ".xml", ".json", ".yaml", ".yml", ".in", ".idl", ".thrift", ".fbs", ".def")
    if lower_path.endswith(template_exts):
        return True, "template"

    # Generator scripts
    script_exts = (".py", ".pl", ".sh", ".bash", ".bat", ".cmake")
    if lower_path.endswith(script_exts) or basename == "cmakelists.txt":
        return True, "script"

    # Generated intermediates
    intermediate_exts = (".pb.cc", ".pb.h", ".grpc.pb.cc", ".grpc.pb.h", "_wrap.cxx", "_wrap.cpp", ".moc.cpp")
    if lower_path.endswith(intermediate_exts):
        return True, "intermediate"

    # Exclude build artifacts and stamps
    excluded_exts = (".o", ".obj", ".a", ".lib", ".so", ".dll", ".stamp", ".d", ".ninja", ".ninja_deps", ".ninja_log")
    if lower_path.endswith(excluded_exts) or basename == "build.ninja":
        return False, "excluded"

    return False, "unknown"


def parse_ninja_explain_line(line: str, explain_pattern: re.Pattern[str]) -> Optional[Tuple[str, str]]:
    """Parse a single line from ninja explain output.

    Args:
        line: Line from ninja explain output
        explain_pattern: Regex pattern to match explain messages

    Returns:
        Tuple of (output_file, explain_msg) or None if line doesn't match
    """
    m = explain_pattern.search(line)
    if not m:
        return None

    explain_msg = m.group(1)

    # Skip "is dirty" lines - these are just CMake files
    if "is dirty" in explain_msg:
        return None

    output_file = "unknown"
    if explain_msg.startswith("output "):
        parts = explain_msg.split(" ", 2)
        if len(parts) > 1:
            output_file = parts[1]
    elif "command line changed for " in explain_msg:
        output_file = explain_msg.split("command line changed for ", 1)[1]

    return (output_file, explain_msg)


def check_ninja_available() -> bool:
    """Check if ninja is available in PATH.

    Returns:
        True if ninja is available, False otherwise
    """
    tool_info = find_ninja()
    return tool_info.is_found()


def validate_build_directory(build_dir: str) -> Path:
    """Validate that the build directory exists and contains a ninja build file.

    Args:
        build_dir: Path to the build directory

    Returns:
        Validated Path object

    Raises:
        ValueError: If directory is invalid or doesn't contain build.ninja
    """
    path = Path(build_dir).resolve()

    if not path.exists():
        raise ValueError(f"Build directory does not exist: {build_dir}")

    if not path.is_dir():
        raise ValueError(f"Path is not a directory: {build_dir}")

    build_ninja = path / "build.ninja"
    if not build_ninja.exists():
        raise ValueError(f"No build.ninja found in {build_dir}. " "Please provide a valid ninja build directory.")

    return path


def parse_ninja_generated_files(build_ninja_path: str) -> Tuple[Set[str], Dict[str, GeneratedFileInfo]]:
    """Parse build.ninja to identify files that are generated by build rules and their dependencies.

    Generated files are identified by:
    - Being outputs of CUSTOM_COMMAND rules (CMake generated files)
    - Being outputs of rules with 'generator = 1' flag
    - Being outputs of code generation rules

    Also tracks input dependencies (templates, scripts, configs) for each generated file.

    Args:
        build_ninja_path: Path to build.ninja file

    Returns:
        Tuple of:
        - Set of all files to track (outputs + inputs)
        - Dict mapping output files to their GeneratedFileInfo
    """
    all_tracked_files: Set[str] = set()
    output_to_info: Dict[str, GeneratedFileInfo] = {}

    # Track generator rules (rules with generator = 1)
    generator_rules: Set[str] = set()
    current_rule: Optional[str] = None

    # Regex patterns - enhanced to capture inputs
    rule_pattern = re.compile(r"^rule\s+(\S+)")
    generator_pattern = re.compile(r"^\s+generator\s*=\s*1")
    build_pattern = re.compile(r"^build\s+([^:]+):\s+(\S+)(?:\s+(.+))?$")

    logger.debug("Parsing %s to identify generated files and dependencies...", build_ninja_path)

    try:
        with open(build_ninja_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.rstrip()

                # Check for rule definition
                rule_match = rule_pattern.match(line)
                if rule_match:
                    current_rule = rule_match.group(1)
                    continue

                # Check if current rule is a generator rule
                if current_rule and generator_pattern.match(line):
                    generator_rules.add(current_rule)
                    logger.debug("Found generator rule: %s", current_rule)
                    continue

                # Check for build statements
                build_match = build_pattern.match(line)
                if build_match:
                    outputs_str = build_match.group(1)
                    rule_name = build_match.group(2)
                    inputs_str = build_match.group(3) if build_match.group(3) is not None else ""

                    # Mark outputs as generated if:
                    # 1. Rule is a generator rule
                    # 2. Rule is CUSTOM_COMMAND (CMake generated files)
                    # 3. Rule is not a C/C++ compilation rule
                    if rule_name in generator_rules or "CUSTOM_COMMAND" in rule_name or rule_name in {"phony", "RERUN_CMAKE"}:

                        # Parse output files (handle | for implicit outputs)
                        outputs = outputs_str.split("|")[0].strip().split()

                        # Parse input dependencies: format is "explicit [| implicit] [|| order-only]"
                        explicit_inputs_list: List[str] = []
                        implicit_inputs_list: List[str] = []
                        order_only_inputs_list: List[str] = []

                        if inputs_str:
                            # Split by || for order-only deps
                            parts = inputs_str.split("||")
                            deps_part = parts[0].strip()
                            order_only_part = parts[1].strip() if len(parts) > 1 else ""

                            # Split deps by | for implicit deps
                            dep_sections = deps_part.split("|")
                            explicit_str = dep_sections[0].strip()
                            implicit_str = dep_sections[1].strip() if len(dep_sections) > 1 else ""

                            # Parse explicit inputs
                            if explicit_str:
                                explicit_inputs_list = [inp.strip() for inp in explicit_str.split() if inp.strip()]

                            # Parse implicit inputs
                            if implicit_str:
                                implicit_inputs_list = [inp.strip() for inp in implicit_str.split() if inp.strip()]

                            # Parse order-only inputs
                            if order_only_part:
                                order_only_inputs_list = [inp.strip() for inp in order_only_part.split() if inp.strip()]

                        # Track outputs and create GeneratedFileInfo for each
                        for output in outputs:
                            output = output.strip()
                            if output and not output.endswith(".stamp"):
                                # Check if output is trackable
                                is_trackable, category = is_trackable_file(output)

                                # For phony/RERUN_CMAKE targets, track even if extension not recognized
                                if is_trackable or rule_name in {"phony", "RERUN_CMAKE"}:
                                    all_tracked_files.add(output)
                                    if is_trackable:
                                        logger.debug("Identified generated file: %s (rule: %s, category: %s)", output, rule_name, category)
                                    else:
                                        logger.debug("Identified generated file: %s (rule: %s, no extension)", output, rule_name)

                                # Track inputs (templates, scripts, configs)
                                trackable_inputs: List[str] = []
                                for inp in explicit_inputs_list + implicit_inputs_list:
                                    inp_trackable, inp_category = is_trackable_file(inp)
                                    if inp_trackable:
                                        all_tracked_files.add(inp)
                                        trackable_inputs.append(inp)
                                        logger.debug("  Input dependency: %s (category: %s)", inp, inp_category)

                                # Store GeneratedFileInfo
                                output_to_info[output] = GeneratedFileInfo(
                                    outputs={output},
                                    explicit_inputs=set(explicit_inputs_list),
                                    implicit_inputs=set(implicit_inputs_list),
                                    order_only_inputs=set(order_only_inputs_list),
                                    rule_name=rule_name,
                                )

    except IOError as e:
        logger.warning("Failed to read %s: %s", build_ninja_path, e)
        return set(), {}

    logger.info("Found %d generated files (%d total tracked files including inputs) in build.ninja", len(output_to_info), len(all_tracked_files))
    return all_tracked_files, output_to_info


def compute_file_hash(file_path: str) -> str:
    """Compute SHA256 hash of a file.

    Args:
        file_path: Path to the file

    Returns:
        Hexadecimal hash string
    """
    sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                sha256.update(chunk)
        return sha256.hexdigest()
    except IOError as e:
        logger.warning("Failed to hash %s: %s", file_path, e)
        return ""


def load_generated_files_cache(cache_path: str) -> Dict[str, Any]:
    """Load the cache of generated file hashes and metadata.

    Cache structure:
    {
        "build_ninja_mtime": float,
        "files": {path: hash, ...},
        "dependencies": {output: [inputs], ...}
    }

    Args:
        cache_path: Path to the cache file

    Returns:
        Dictionary with cache metadata and file hashes
    """
    if not os.path.exists(cache_path):
        return {"build_ninja_mtime": 0.0, "files": {}, "dependencies": {}}

    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            # Support old cache format (just dict of paths->hashes)
            if isinstance(data, dict) and "files" not in data:
                return {"build_ninja_mtime": 0.0, "files": data, "dependencies": {}}
            # Cast to expected type for mypy
            result: Dict[str, Any] = data if isinstance(data, dict) else {"build_ninja_mtime": 0.0, "files": {}, "dependencies": {}}
            return result
    except (IOError, json.JSONDecodeError) as e:
        logger.warning("Failed to load generated files cache: %s", e)
        return {"build_ninja_mtime": 0.0, "files": {}, "dependencies": {}}


def save_generated_files_cache(cache_path: str, cache: Dict[str, Any]) -> None:
    """Save the cache of generated file hashes and metadata.

    Args:
        cache_path: Path to the cache file
        cache: Cache dictionary with metadata and file hashes
    """
    try:
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache, f, indent=2)
    except IOError as e:
        logger.warning("Failed to save generated files cache: %s", e)


def check_generated_files_changed(
    build_dir: str, generated_files: Set[str], cache: Dict[str, Any], output_to_info: Optional[Dict[str, GeneratedFileInfo]] = None
) -> Tuple[List[str], List[str], Dict[str, str]]:
    """Check if generated files or their inputs are missing or have changed since last cache.

    Args:
        build_dir: Path to the build directory
        generated_files: Set of generated file paths (relative to build_dir)
        cache: Cache dictionary with metadata and file path -> hash mappings
        output_to_info: Optional mapping of outputs to their GeneratedFileInfo for input tracking

    Returns:
        Tuple of (missing_files, changed_files, change_reasons)
        where change_reasons maps file path -> reason for change
    """
    missing_files = []
    changed_files = []
    change_reasons: Dict[str, str] = {}  # Maps output path -> reason for change

    # Extract file hashes from new cache structure
    file_hashes = cache.get("files", {}) if isinstance(cache, dict) else cache

    # Track which inputs changed (for better error messages)
    changed_inputs: Dict[str, str] = {}  # Maps input path -> change type

    # First pass: Check all tracked files for changes
    for gen_file in generated_files:
        # Check all trackable files now (not just C/C++ sources)
        is_trackable, category = is_trackable_file(gen_file)
        if not is_trackable:
            continue

        full_path = os.path.join(build_dir, gen_file) if not os.path.isabs(gen_file) else gen_file

        if not os.path.exists(full_path):
            missing_files.append(full_path)
            change_reasons[full_path] = f"missing {category}"
            logger.debug("Missing generated file: %s (%s)", full_path, category)

            # If this is an input file that's missing, track it
            if category in {"template", "script"}:
                changed_inputs[full_path] = "missing"
        else:
            # Check if hash has changed (only if file was previously cached)
            cached_hash = file_hashes.get(full_path)

            if cached_hash is not None:
                # File was in cache, check if it changed
                current_hash = compute_file_hash(full_path)
                if current_hash != cached_hash:
                    changed_files.append(full_path)
                    change_reasons[full_path] = f"modified {category}"
                    logger.debug("Generated file changed: %s (%s)", full_path, category)

                    # If this is an input file that changed, track it
                    if category in {"template", "script"}:
                        changed_inputs[full_path] = "modified"
            # File not in cache - new file that doesn't need rebuild (cache is valid in this path)

    # Second pass: Check if any outputs depend on changed inputs
    if output_to_info and changed_inputs:
        for output_path, file_info in output_to_info.items():
            full_output_path = os.path.join(build_dir, output_path) if not os.path.isabs(output_path) else output_path

            # Skip if already marked for rebuild
            if full_output_path in missing_files or full_output_path in changed_files:
                continue

            # Check all input dependencies
            all_inputs = list(file_info.explicit_inputs) + list(file_info.implicit_inputs)

            for input_file in all_inputs:
                full_input_path = os.path.join(build_dir, input_file) if not os.path.isabs(input_file) else input_file

                if full_input_path in changed_inputs:
                    changed_files.append(full_output_path)
                    change_type = changed_inputs[full_input_path]
                    input_basename = os.path.basename(input_file)
                    change_reasons[full_output_path] = f"input {change_type}: {input_basename}"
                    logger.info("Output %s needs rebuild: input %s %s", os.path.basename(output_path), input_basename, change_type)
                    break  # Only need one reason per output

    return missing_files, changed_files, change_reasons


def update_generated_files_cache(cache: Dict[str, Any], files_to_update: List[str], cache_path: str) -> None:
    """Update the cache with current hashes of specific generated files.

    Args:
        cache: Current cache dictionary to update
        files_to_update: List of file paths to update in cache
        cache_path: Path to save the cache file
    """
    # Ensure cache has proper structure
    if "files" not in cache:
        cache["files"] = {}

    file_hashes = cache["files"]

    for full_path in files_to_update:
        if os.path.exists(full_path):
            file_hashes[full_path] = compute_file_hash(full_path)

    save_generated_files_cache(cache_path, cache)


def clean_stale_cache_entries(cache: Dict[str, Any], current_generated_files: Set[str], build_dir: str) -> Dict[str, Any]:
    """Remove cache entries for files that are no longer generated or don't exist.

    Args:
        cache: Current cache dictionary
        current_generated_files: Set of currently generated file paths (relative to build_dir)
        build_dir: Build directory path

    Returns:
        Cleaned cache dictionary with only valid entries
    """
    # Ensure cache has proper structure
    if "files" not in cache:
        return cache

    file_hashes = cache["files"]

    # Build set of expected absolute paths from current generated files
    expected_paths: Set[str] = set()
    for gen_file in current_generated_files:
        # Track all trackable files now
        is_trackable, _ = is_trackable_file(gen_file)
        if is_trackable:
            full_path = os.path.join(build_dir, gen_file) if not os.path.isabs(gen_file) else gen_file
            expected_paths.add(full_path)

    # Create new cache with only valid entries
    cleaned_file_hashes: Dict[str, str] = {}
    removed_count = 0

    for file_path, file_hash in file_hashes.items():
        # Keep entry if file is still in generated files list and exists
        if file_path in expected_paths and os.path.exists(file_path):
            cleaned_file_hashes[file_path] = file_hash
        else:
            removed_count += 1
            logger.debug("Removing stale cache entry: %s", file_path)

    if removed_count > 0:
        logger.info("Cleaned %s stale entries from generated files cache", removed_count)

    # Update cache with cleaned files
    cache["files"] = cleaned_file_hashes
    return cache


def check_missing_source_files(compile_commands_path: str) -> List[str]:
    """Check if any source files referenced in compile_commands.json are missing.

    This detects when autogenerated files haven't been built yet.

    Args:
        compile_commands_path: Path to compile_commands.json

    Returns:
        List of missing file paths (empty if all files exist)

    Raises:
        RuntimeError: If compile_commands.json is invalid
    """
    try:
        with open(compile_commands_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (IOError, json.JSONDecodeError) as e:
        raise RuntimeError(f"Failed to read compile_commands.json: {e}") from e

    if not isinstance(data, list):
        raise RuntimeError(f"Invalid compile_commands.json format: expected list, got {type(data)}")

    missing_files = []
    for entry in data:
        if not isinstance(entry, dict):
            continue

        file_path = entry.get("file")
        if not file_path:
            continue

        # Handle relative paths by resolving against the build directory
        directory = entry.get("directory", "")
        if directory and not os.path.isabs(file_path):
            full_path = os.path.join(directory, file_path)
        else:
            full_path = file_path

        if not os.path.exists(full_path):
            missing_files.append(full_path)
            logger.debug("Missing source file: %s", full_path)

    return missing_files


def run_full_ninja_build(build_dir: str, verbose: bool = False, timeout: int = 3600 * 3, targets: Optional[List[str]] = None) -> bool:
    """Run a ninja build to generate autogenerated files.

    Args:
        build_dir: Path to the build directory
        verbose: Whether to print progress messages
        timeout: Command timeout in seconds (default: 3600 * 3 = 3 hours)
        targets: Optional list of specific targets to build. If None, builds all.

    Returns:
        True if build succeeded, False otherwise
    """
    if targets:
        logger.info("Running ninja build for %s targets in %s", len(targets), build_dir)
        if verbose:
            print_warning(f"Building {len(targets)} autogenerated files...", prefix=False)
    else:
        logger.info("Running full ninja build in %s", build_dir)
        if verbose:
            print_warning("Running full ninja build to generate autogenerated files...", prefix=False)

    try:
        # Build command: ninja [target1 target2 ...]
        cmd = ["ninja"]
        if targets:
            cmd.extend(targets)

        # Run ninja with real-time output (no capture_output) so progress is visible
        result = subprocess.run(cmd, cwd=build_dir, timeout=timeout)

        if result.returncode == 0:
            logger.info("Ninja build completed successfully")
            if verbose:
                print_success("Build completed successfully")
            return True

        logger.warning("Ninja build failed with exit code %s", result.returncode)
        if verbose:
            print_error(f"Build failed with exit code {result.returncode}", prefix=False)
        return False

    except subprocess.TimeoutExpired:
        logger.error("Ninja build timed out after %s seconds", timeout)
        if verbose:
            print_error(f"Build timed out after {timeout} seconds", prefix=False)
        return False
    except FileNotFoundError:
        logger.error("ninja command not found")
        if verbose:
            print_error("ninja command not found in PATH", prefix=False)
        return False
    except Exception as e:
        logger.error("Unexpected error running ninja build: %s", e)
        if verbose:
            print_error(f"Unexpected error: {e}", prefix=False)
        return False


def get_relative_build_path(full_path: str, build_dir: str) -> str:
    """Convert an absolute file path to a path relative to build directory.

    This is needed because ninja expects targets relative to the build directory.

    Args:
        full_path: Absolute path to a file
        build_dir: Build directory path

    Returns:
        Path relative to build directory, or the original path if conversion fails
    """
    try:
        rel_path = os.path.relpath(full_path, build_dir)
        # If path goes outside build_dir, return the basename
        if rel_path.startswith(".."):
            return os.path.basename(full_path)
        return rel_path
    except ValueError:
        # On Windows, relpath can fail if paths are on different drives
        return os.path.basename(full_path)


def validate_and_prepare_build_dir(build_dir: str, verbose: bool = False) -> Tuple[str, str]:
    """Validate build directory and ensure compile_commands.json exists.

    Args:
        build_dir: Path to ninja build directory
        verbose: Whether to print progress messages

    Returns:
        Tuple of (validated build_dir path, compile_commands.json path)

    Raises:
        ValueError: If build_dir is invalid or path traversal detected
        RuntimeError: If compile_commands.json generation fails
    """
    build_dir = os.path.realpath(os.path.abspath(build_dir))
    if not os.path.isdir(build_dir):
        raise ValueError(f"Build directory does not exist: {build_dir}")

    compile_commands = os.path.realpath(os.path.join(build_dir, COMPILE_COMMANDS_JSON))
    build_ninja = os.path.realpath(os.path.join(build_dir, "build.ninja"))

    # Validate paths are within build_dir (protect against symlink attacks)
    try:
        rel_path = os.path.relpath(compile_commands, build_dir)
        if rel_path.startswith(".."):
            raise ValueError()
    except (ValueError, OSError) as exc:
        raise ValueError(f"Path traversal detected: {COMPILE_COMMANDS_JSON}") from exc

    # Parse build.ninja to identify generated files and their dependencies
    all_tracked_files, output_to_info = parse_ninja_generated_files(build_ninja)

    # Load cache of previous generated file hashes
    cache_path = os.path.join(build_dir, GENERATED_FILES_CACHE)
    cache = load_generated_files_cache(cache_path)

    # Check if build.ninja has changed - invalidate cache if it has
    build_ninja_mtime = os.path.getmtime(build_ninja) if os.path.exists(build_ninja) else 0.0
    cached_build_ninja_mtime = cache.get("build_ninja_mtime", 0.0)

    cache_was_invalidated = False
    if build_ninja_mtime != cached_build_ninja_mtime:
        logger.info("build.ninja changed (mtime: %.2f vs cached: %.2f), invalidating cache", build_ninja_mtime, cached_build_ninja_mtime)
        if verbose:
            print_warning("Build rules changed, invalidating generated files cache...", prefix=False)
        # Clear file hashes but keep structure
        cache = {"build_ninja_mtime": build_ninja_mtime, "files": {}, "dependencies": {}}
        cache_was_invalidated = True
    else:
        # Update mtime in cache
        cache["build_ninja_mtime"] = build_ninja_mtime

    # Clean stale entries from cache (removed files)
    cache = clean_stale_cache_entries(cache, all_tracked_files, build_dir)

    # Check if cache is empty (first run or just invalidated)
    cache_is_empty = len(cache.get("files", {})) == 0

    # Store dependency mapping in cache for input change detection
    dependencies_map: Dict[str, List[str]] = {}
    for output_path, file_info in output_to_info.items():
        # Store all inputs (explicit + implicit) for each output
        all_inputs = list(file_info.explicit_inputs) + list(file_info.implicit_inputs)
        if all_inputs:
            dependencies_map[output_path] = all_inputs
    cache["dependencies"] = dependencies_map

    # When cache is invalid/empty, delegate to ninja to rebuild all generated file targets
    # Ninja will use its own dependency tracking to determine what actually needs rebuilding,
    # including any C/C++ generator executables, while skipping unrelated compilation
    if cache_was_invalidated or cache_is_empty:
        logger.info("Cache invalid - delegating to ninja for all generated file targets")
        if verbose:
            if cache_was_invalidated:
                print_warning("Delegating to ninja to verify all generated files...", prefix=False)
            else:
                print_warning("No cache found, delegating to ninja to build generated files...", prefix=False)

        # Collect all trackable generated file output targets
        generated_output_targets = [output_path for output_path in output_to_info.keys() if is_trackable_file(output_path)[0]]

        if generated_output_targets:
            logger.info("Building %d generated file targets via ninja", len(generated_output_targets))

            if not run_full_ninja_build(build_dir, verbose=verbose, targets=generated_output_targets):
                logger.warning("Ninja build failed, but continuing anyway")
                if verbose:
                    print_error("Build failed - some generated files may be incomplete")
            else:
                if verbose:
                    print_success("Generated files build completed")

            # Update cache with all tracked files after build
            all_tracked_paths = [os.path.join(build_dir, f) if not os.path.isabs(f) else f for f in all_tracked_files if is_trackable_file(f)[0]]
            update_generated_files_cache(cache, all_tracked_paths, cache_path)

        files_to_build = []  # Already handled by ninja
    else:
        # Cache is valid - use incremental hash-based change detection
        # Check if generated files are missing or changed (including input dependencies)
        missing_generated, changed_generated, change_reasons = check_generated_files_changed(build_dir, all_tracked_files, cache, output_to_info)

        # If any generated files are missing or changed, build only those files
        files_to_build = list(set(missing_generated + changed_generated))

        if files_to_build:
            logger.info("Found %s missing and %s changed generated files", len(missing_generated), len(changed_generated))

            if verbose:
                print_warning(f"Detected {len(files_to_build)} autogenerated files needing rebuild...", prefix=False)

                # Show detailed change reasons
                if change_reasons:
                    # Group by reason for cleaner output
                    reason_groups: Dict[str, List[str]] = {}
                    for file_path, reason in change_reasons.items():
                        if reason not in reason_groups:
                            reason_groups[reason] = []
                        reason_groups[reason].append(os.path.basename(file_path))

                    for reason, files in sorted(reason_groups.items()):
                        file_list = ", ".join(files[:5])  # Show first 5
                        if len(files) > 5:
                            file_list += f" (+{len(files)-5} more)"
                        print(f"  {Colors.YELLOW}â€¢{Colors.RESET} {reason}: {file_list}")

            # Convert absolute paths to build-relative paths for ninja
            targets = [get_relative_build_path(f, build_dir) for f in files_to_build]

            if not run_full_ninja_build(build_dir, verbose=verbose, targets=targets):
                logger.warning("Ninja build failed, but continuing anyway")
                if verbose:
                    print_error("Build failed - some generated files may be incomplete")
            else:
                # Post-build verification: Check that all expected outputs now exist
                verification_failed = False
                for file_path in files_to_build:
                    if not os.path.exists(file_path):
                        verification_failed = True
                        reason = change_reasons.get(file_path, "unknown reason")
                        logger.error("Post-build verification failed: %s still missing after build (reason: %s)", file_path, reason)
                        if verbose:
                            print_error(f"Verification failed: {os.path.basename(file_path)} not generated")

                if verification_failed:
                    logger.warning("Some generated files missing after build - cache not updated for failed files")
                    # Only update cache for files that actually exist
                    files_to_build = [f for f in files_to_build if os.path.exists(f)]

            # Update cache with new hashes after build (only for successfully generated files)
            if files_to_build:
                update_generated_files_cache(cache, files_to_build, cache_path)
        else:
            logger.info("No changes detected in generated files")
            if verbose:
                print_success("Autogenerated files are up to date")

    # Determine if we need to regenerate compile_commands.json
    need_build = len(files_to_build) > 0

    # Generate compile_commands.json if needed
    needs_regeneration = (
        not os.path.exists(compile_commands)
        or (os.path.exists(build_ninja) and os.path.getmtime(build_ninja) > os.path.getmtime(compile_commands))
        or need_build
    )

    if needs_regeneration:
        logger.info("Generating compile_commands.json...")
        if verbose:
            print(f"{Colors.CYAN}Generating compile_commands.json...{Colors.RESET}")
        try:
            result = subprocess.run(["ninja", "-t", "compdb"], capture_output=True, text=True, cwd=build_dir, check=True)
            with open(compile_commands, "w", encoding="utf-8") as f:
                f.write(result.stdout)
            logger.debug("Generated: %s", compile_commands)

            # Update cache after successful generation with all tracked files
            if all_tracked_files:
                # Convert set to list for all tracked files
                all_generated = [
                    os.path.join(build_dir, f) if not os.path.isabs(f) else f
                    for f in all_tracked_files
                    if any(f.endswith(ext) for ext in [".cpp", ".cc", ".cxx", ".c", ".h", ".hpp", ".hxx"])
                ]
                update_generated_files_cache(cache, all_generated, cache_path)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Failed to generate compile_commands.json: {e.stderr}") from e
        except IOError as e:
            raise IOError(f"Failed to write compile_commands.json: {e}") from e

    if not os.path.isfile(compile_commands):
        raise ValueError(f"compile_commands.json not found in {build_dir}")

    return build_dir, compile_commands


def validate_build_directory_with_feedback(build_directory: str, verbose: bool = False) -> Tuple[str, str]:
    """Validate and prepare build directory with user-friendly error messages.

    This is a wrapper around validate_and_prepare_build_dir that provides
    standardized error handling and user feedback for command-line scripts.

    Args:
        build_directory: Path to build directory to validate
        verbose: Whether to print verbose messages

    Returns:
        Tuple of (validated build_dir path, compile_commands.json path)

    Raises:
        ValueError: If validation fails
        RuntimeError: If preparation fails
    """
    # Check for build.ninja first (more specific error message)
    build_dir_check = os.path.realpath(os.path.abspath(build_directory))
    build_ninja = os.path.realpath(os.path.join(build_dir_check, "build.ninja"))
    if not os.path.isfile(build_ninja):
        error_msg = f"build.ninja not found in '{build_dir_check}'"
        logger.error(error_msg)
        print_error(error_msg)
        print_warning("Hint: This script requires a Ninja build directory", prefix=False)
        raise ValueError(error_msg)

    # Validate and prepare build directory
    try:
        build_dir, compile_commands = validate_and_prepare_build_dir(build_directory, verbose=verbose)
        logger.info("Using build directory: %s", build_dir)
        return build_dir, compile_commands
    except (ValueError, RuntimeError) as e:
        # Don't print error again, just log and re-raise
        logger.error("Build directory validation failed: %s", e)
        raise
    except Exception as e:
        logger.exception("Error validating build directory: %s", e)
        raise RuntimeError(f"Error validating build directory: {e}") from e


def run_ninja_explain(build_dir: Path, timeout: int = 300) -> subprocess.CompletedProcess[str]:
    """Run ninja -n -d explain to get rebuild information.

    Args:
        build_dir: Path to the build directory
        timeout: Command timeout in seconds (default: 300)

    Returns:
        CompletedProcess object with ninja output

    Raises:
        RuntimeError: If ninja execution fails
    """
    logger.info("Running ninja -n -d explain in %s", build_dir)

    try:
        result = subprocess.run(["ninja", "-n", "-d", "explain"], capture_output=True, text=True, cwd=str(build_dir), timeout=timeout)
        return result
    except FileNotFoundError as exc:
        raise RuntimeError("ninja command not found. Please ensure ninja is installed and in PATH.") from exc
    except subprocess.TimeoutExpired as exc:
        raise RuntimeError(f"ninja command timed out after {timeout} seconds") from exc
    except Exception as e:
        raise RuntimeError(f"Unexpected error running ninja: {e}") from e


def parse_ninja_explain_output(stderr_lines: List[str]) -> Tuple[List[str], Set[str]]:
    """Parse ninja explain output to extract rebuild targets and changed files.

    Args:
        stderr_lines: Lines from ninja stderr output

    Returns:
        Tuple of (rebuild_targets, changed_files)
    """
    rebuild_targets = []
    changed_files = set()

    for line in stderr_lines:
        match = RE_NINJA_EXPLAIN.search(line)
        if not match:
            continue

        explain_msg = match.group(1)

        # Skip "is dirty" lines as they're redundant
        if "is dirty" in explain_msg:
            continue

        # Extract target from the message
        if explain_msg.startswith("output "):
            parts = explain_msg.split(" ", 2)
            if len(parts) > 1:
                target = parts[1]
                rebuild_targets.append(target)
        elif "command line changed for " in explain_msg:
            target = explain_msg.split("command line changed for ", 1)[1]
            rebuild_targets.append(target)

        # Extract changed files (headers)
        input_match = RE_RECENT_INPUT.search(explain_msg)
        if input_match:
            changed_file = input_match.group(1)
            changed_files.add(changed_file)

    return rebuild_targets, changed_files


def extract_rebuild_info(build_dir: str, verbose: bool = False) -> Tuple[List[Tuple[str, str]], Dict[str, int], Dict[str, int]]:
    """Extract rebuild information from ninja explain output.

    Args:
        build_dir: Path to the ninja build directory
        verbose: If True, print detailed progress information

    Returns:
        tuple: (rebuild_entries, reasons, root_causes) where:
            - rebuild_entries: list of (output_file, reason) tuples
            - reasons: dict mapping normalized reason to count
            - root_causes: dict mapping changed file to rebuild count

    Raises:
        BuildDirectoryError: If build directory is invalid or inaccessible
        NinjaError: If ninja command fails, times out, or is not found
        RuntimeError: If an unexpected error occurs
    """
    if verbose:
        print(f"Analyzing build directory: {build_dir}", file=sys.stderr)

    # Save current directory and change to build directory
    original_dir = os.getcwd()
    try:
        os.chdir(build_dir)
    except OSError as e:
        print(f"Error: Cannot change to directory '{build_dir}': {e}", file=sys.stderr)
        raise BuildDirectoryError(f"Cannot change to directory '{build_dir}': {e}") from e

    try:
        result = subprocess.run(["ninja", "-n", "-d", "explain"], capture_output=True, text=True, check=True, timeout=300)
    except FileNotFoundError as exc:
        print("Error: 'ninja' command not found. Please ensure ninja is installed and in PATH.", file=sys.stderr)
        raise NinjaError("ninja command not found. Please ensure ninja is installed and in PATH.") from exc
    except subprocess.TimeoutExpired as exc:
        print("Error: Ninja command timed out after 5 minutes.", file=sys.stderr)
        raise NinjaError("Ninja command timed out after 5 minutes") from exc
    except subprocess.CalledProcessError as e:
        print(f"Error: Ninja command failed with exit code {e.returncode}", file=sys.stderr)
        if e.stderr:
            print(f"Stderr output:\n{e.stderr}", file=sys.stderr)
        raise NinjaError(f"Ninja command failed with exit code {e.returncode}") from e
    except Exception as e:
        print(f"Unexpected error running ninja: {e}", file=sys.stderr)
        raise RuntimeError(f"Unexpected error running ninja: {e}") from e
    finally:
        os.chdir(original_dir)

    lines = result.stderr.splitlines()

    if verbose:
        print(f"Processing {len(lines)} lines of ninja output", file=sys.stderr)

    rebuild_entries = []
    reasons: Dict[str, int] = defaultdict(int)
    root_causes: Dict[str, int] = defaultdict(int)

    for line in lines:
        parsed = parse_ninja_explain_line(line, RE_NINJA_EXPLAIN)
        if not parsed:
            continue

        output_file, explain_msg = parsed

        reason_norm = normalize_reason(explain_msg)
        rebuild_entries.append((output_file, reason_norm))
        reasons[reason_norm] += 1

        m2 = re.search(r"([^\s]+\.h\w*)", explain_msg)
        if m2:
            root_causes[m2.group(1)] += 1

    return rebuild_entries, reasons, root_causes


def normalize_reason(msg: str) -> str:
    """Normalize a ninja explain message into a human-readable rebuild reason.

    Args:
        msg: Raw ninja explain message

    Returns:
        Normalized, user-friendly reason string
    """
    if not msg:
        return "unknown reason"

    msg_lower = msg.lower()

    if "output missing" in msg_lower or "doesn't exist" in msg_lower:
        return "output missing (initial build)"
    if "older than most recent input" in msg_lower:
        return "input source changed"
    if "command line changed" in msg_lower:
        return "command line changed (compile flags/options)"
    if "input" in msg_lower and "newer" in msg_lower:
        return "input source changed"
    if "depfile" in msg_lower:
        return "header dependency changed"
    if "build.ninja" in msg_lower:
        return "build.ninja changed (cmake reconfigure)"
    if "rule changed" in msg_lower:
        return "rule changed (compile flags/options)"
    if "is dirty" in msg_lower:
        return "[marked dirty]"

    return msg


def get_dependencies(build_dir: str, target: str, timeout: int = 30) -> List[str]:
    """Get dependencies for a target using ninja -t deps.

    Args:
        build_dir: Path to the build directory
        target: The build target to get dependencies for
        timeout: Command timeout in seconds (default: 30)

    Returns:
        List of dependency file paths

    Raises:
        RuntimeError: If ninja command fails or times out
    """
    try:
        result = subprocess.run(["ninja", "-t", "deps", target], capture_output=True, text=True, cwd=build_dir, timeout=timeout)

        if result.returncode != 0:
            logger.warning("ninja -t deps failed for %s: %s", target, result.stderr)
            return []

        # Parse the output
        deps = []
        for line in result.stdout.splitlines():
            line = line.strip()
            # Skip target line (ends with :), empty lines, and comments
            if line and not line.endswith(":") and not line.startswith("#"):
                deps.append(line)

        return deps

    except subprocess.TimeoutExpired:
        logger.error("ninja -t deps timed out for %s", target)
        return []
    except subprocess.CalledProcessError as e:
        logger.error("ninja -t deps failed for %s: %s", target, e)
        return []
    except Exception as e:
        logger.error("Unexpected error getting dependencies for %s: %s", target, e)
        return []


def generate_compile_commands(build_dir: str, timeout: int = 60) -> bool:
    """Generate compile_commands.json using ninja -t compdb.

    Args:
        build_dir: Path to the build directory
        timeout: Command timeout in seconds (default: 60)

    Returns:
        True if successful, False otherwise
    """
    compile_db = os.path.join(build_dir, COMPILE_COMMANDS_JSON)

    try:
        logger.info("Generating compile_commands.json...")
        result = subprocess.run(["ninja", "-t", "compdb"], capture_output=True, text=True, cwd=build_dir, check=True, timeout=timeout)

        with open(compile_db, "w", encoding="utf-8") as f:
            f.write(result.stdout)

        logger.debug("Generated: %s", compile_db)
        return True

    except subprocess.TimeoutExpired:
        logger.error("ninja -t compdb timed out after %s seconds", timeout)
        return False
    except subprocess.CalledProcessError as e:
        logger.error("Failed to generate compile_commands.json: %s", e.stderr)
        return False
    except IOError as e:
        logger.error("Failed to write compile_commands.json: %s", e)
        return False
    except Exception as e:
        logger.error("Unexpected error generating compile_commands.json: %s", e)
        return False
